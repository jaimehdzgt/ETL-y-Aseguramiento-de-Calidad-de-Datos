{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaimehdzgt/ETL-y-Aseguramiento-de-Calidad-de-Datos/blob/main/etl_cleaning_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d747d8e",
      "metadata": {
        "id": "3d747d8e"
      },
      "source": [
        "\n",
        "# Proyecto ETL End-to-End: Limpieza y Generación de Insights desde Datos Sucios\n",
        "\n",
        "**Objetivo:** Convertir el dataset `etl_dirty_orders.xlsx` en una tabla confiable para análisis, documentando reglas de limpieza, validación y enriquecimiento; y entregar un dataset final + reporte de calidad + visualizaciones clave.\n",
        "\n",
        "> **Instrucciones:** Coloca el archivo **`etl_dirty_orders.xlsx`** en la misma carpeta del notebook (o ajusta la variable `DATA_PATH`). El archivo incluye:\n",
        "> - Hoja **`raw_orders`** (datos sucios)\n",
        "> - Hoja **`lookup_states`** (normalización de estados)\n",
        "> - Hoja **`lookup_subcats`** (corrección de subcategorías)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72709d64",
      "metadata": {
        "id": "72709d64"
      },
      "source": [
        "## 1) Configuración e importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce9e9d24",
      "metadata": {
        "id": "ce9e9d24"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, re\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "plt.rcParams['figure.figsize'] = (8, 5)\n",
        "\n",
        "DATA_PATH = 'etl_dirty_orders.xlsx'  # coloca el Excel junto al .ipynb\n",
        "SHEET_RAW = 'raw_orders'\n",
        "SHEET_STATES = 'lookup_states'\n",
        "SHEET_SUBCATS = 'lookup_subcats'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56156874",
      "metadata": {
        "id": "56156874"
      },
      "source": [
        "## 2) Carga del dataset y snapshot inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7fc9223",
      "metadata": {
        "id": "e7fc9223"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Montar Google Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Rutas y hojas ===\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "DATA_PATH     = \"/content/drive/MyDrive/Colab Notebooks/datasets/etl_dirty_orders.xlsx\"  # <-- tu archivo en Drive\n",
        "SHEET_RAW     = \"raw_orders\"\n",
        "SHEET_STATES  = \"lookup_states\"\n",
        "SHEET_SUBCATS = \"lookup_subcats\"\n",
        "\n",
        "# === Carga con validación ===\n",
        "assert os.path.exists(DATA_PATH), f\"No se encontró {DATA_PATH}. Verifica la ruta exacta en tu Drive.\"\n",
        "xls = pd.ExcelFile(DATA_PATH)\n",
        "df            = pd.read_excel(xls, SHEET_RAW)\n",
        "lookup_states = pd.read_excel(xls, SHEET_STATES)\n",
        "lookup_subcats= pd.read_excel(xls, SHEET_SUBCATS)\n",
        "\n",
        "print(\"Forma inicial:\", df.shape)\n",
        "display(df.head(10))\n",
        "\n",
        "nulls = df.isna().sum().sort_values(ascending=False)\n",
        "types = df.dtypes\n",
        "print(\"\\nNulos por columna (top 15):\"); display(nulls.head(15).to_frame(\"nulls\"))\n",
        "print(\"\\nTipos detectados:\"); display(types.to_frame(\"dtype\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852fd826",
      "metadata": {
        "id": "852fd826"
      },
      "source": [
        "\n",
        "## 3) Estandarización de esquema (nombres y columnas duplicadas)\n",
        "\n",
        "- Normalizamos encabezados a `snake_case` y trim de espacios.\n",
        "- Resolución de columnas duplicadas/solapadas (por ejemplo: `TOTAL $` vs `TOTAL`). Guardamos **una** columna final y documentamos la decisión.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2162da66",
      "metadata": {
        "id": "2162da66"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_colname(c: str) -> str:\n",
        "    c = c.strip()\n",
        "    c = c.replace('%','_pct').replace('$','_usd').replace('#','num')\n",
        "    c = re.sub(r'[^0-9a-zA-Z]+', '_', c)\n",
        "    c = re.sub(r'__+', '_', c).strip('_')\n",
        "    return c.lower()\n",
        "\n",
        "df.columns = [normalize_colname(c) for c in df.columns]\n",
        "\n",
        "# Unificar TOTAL si existe variante\n",
        "if 'total_usd' in df.columns and 'total' in df.columns:\n",
        "    df['total'] = df['total'].fillna(df['total_usd'])\n",
        "    df.drop(columns=['total_usd'], inplace=True, errors='ignore')\n",
        "\n",
        "# Unificar posibles duplicados de order_id\n",
        "dup_cols = [c for c in df.columns if c.startswith('order_id') and c != 'order_id']\n",
        "for c in dup_cols:\n",
        "    df['order_id'] = df['order_id'].fillna(df[c]) if 'order_id' in df.columns else df[c]\n",
        "    df.drop(columns=[c], inplace=True, errors='ignore')\n",
        "\n",
        "print('Columnas tras normalización:', list(df.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c18bb1aa",
      "metadata": {
        "id": "c18bb1aa"
      },
      "source": [
        "\n",
        "## 4) Limpieza de filas no-datos (SUBTOTAL / TOTAL GENERAL)\n",
        "\n",
        "Removemos filas que son subtotales o totales incrustadas en el cuerpo de la tabla.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3939a426",
      "metadata": {
        "id": "3939a426"
      },
      "outputs": [],
      "source": [
        "\n",
        "before = df.shape[0]\n",
        "mask_bad = df['order_id'].astype(str).str.upper().isin(['SUBTOTAL', 'TOTAL GENERAL', 'TOTAL_GENERAL'])\n",
        "df = df[~mask_bad].copy()\n",
        "after = df.shape[0]\n",
        "print(f'Filas removidas (no-datos): {before - after}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76db3ed6",
      "metadata": {
        "id": "76db3ed6"
      },
      "source": [
        "\n",
        "## 5) Tipos y formatos (fechas, moneda, porcentajes)\n",
        "\n",
        "- **Fechas:** múltiples formatos → parseo robusto.\n",
        "- **Moneda:** normalizamos a `float` (admite `$`, `MXN`, comas como separador decimal y miles).\n",
        "- **Porcentajes:** unificamos a fracción (0–1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618326d7",
      "metadata": {
        "id": "618326d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_date_series(s: pd.Series) -> pd.Series:\n",
        "    def _parse_one(x):\n",
        "        if pd.isna(x) or str(x).strip()=='' or str(x).lower()=='none':\n",
        "            return pd.NaT\n",
        "        x = str(x).strip()\n",
        "        v = pd.to_datetime(x, errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
        "        if pd.isna(v):\n",
        "            try:\n",
        "                v = pd.to_datetime(x, errors='coerce', format='%m/%d/%y')\n",
        "            except:\n",
        "                v = pd.NaT\n",
        "        return v\n",
        "    return s.apply(_parse_one)\n",
        "\n",
        "money_regex = re.compile(r'[^0-9,.-]')\n",
        "def parse_money_series(s: pd.Series) -> pd.Series:\n",
        "    def _to_float(x):\n",
        "        if pd.isna(x):\n",
        "            return np.nan\n",
        "        x = str(x).strip()\n",
        "        if x.lower() in ['n/a','na','incluido','']:\n",
        "            return 0.0 if x.lower()=='incluido' else np.nan\n",
        "        x = money_regex.sub('', x)\n",
        "        if x.count(',') > 0 and x.count('.') > 0:\n",
        "            if x.rfind(',') > x.rfind('.'):\n",
        "                x = x.replace('.', '').replace(',', '.')\n",
        "            else:\n",
        "                x = x.replace(',', '')\n",
        "        else:\n",
        "            if x.count(',') == 1 and x.count('.') == 0:\n",
        "                x = x.replace(',', '.')\n",
        "            else:\n",
        "                x = x.replace(',', '')\n",
        "        try:\n",
        "            return float(x)\n",
        "        except:\n",
        "            return np.nan\n",
        "    return s.apply(_to_float)\n",
        "\n",
        "def parse_percent_series(s: pd.Series) -> pd.Series:\n",
        "    def _pct(x):\n",
        "        if pd.isna(x):\n",
        "            return np.nan\n",
        "        x = str(x).strip().replace('%','')\n",
        "        try:\n",
        "            v = float(x)\n",
        "        except:\n",
        "            return np.nan\n",
        "        return v/100 if v>1 else (v if 0<=v<=1 else np.nan)\n",
        "    return s.apply(_pct)\n",
        "\n",
        "for col in ['order_date','ship_date','payment_date']:\n",
        "    if col in df.columns:\n",
        "        df[col] = parse_date_series(df[col])\n",
        "\n",
        "for col in ['unit_price','tax','shipping','profit','total']:\n",
        "    if col in df.columns:\n",
        "        df[col] = parse_money_series(df[col])\n",
        "\n",
        "for col in ['discount_pct','discount']:\n",
        "    if col in df.columns:\n",
        "        df[col] = parse_percent_series(df[col])\n",
        "\n",
        "if 'discount_pct' in df.columns and 'discount' in df.columns:\n",
        "    df['discount_pct'] = df['discount_pct'].fillna(df['discount'])\n",
        "    df.drop(columns=['discount'], inplace=True, errors='ignore')\n",
        "elif 'discount' in df.columns and 'discount_pct' not in df.columns:\n",
        "    df.rename(columns={'discount':'discount_pct'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab57299",
      "metadata": {
        "id": "5ab57299"
      },
      "source": [
        "\n",
        "## 6) Limpieza general de texto\n",
        "\n",
        "- `strip()` de espacios, normalización de mayúsculas/minúsculas donde aplique.\n",
        "- Columnas objetivo: nombres, ciudad, estado, email, teléfono, categoría, subcategoría, segmento, dirección.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821dad9b",
      "metadata": {
        "id": "821dad9b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def trim_str(s: pd.Series) -> pd.Series:\n",
        "    return s.astype(str).str.strip().replace({'': np.nan})\n",
        "\n",
        "rename_map = {\n",
        "    'phone': 'phone_num',\n",
        "    'customer_email': 'email',\n",
        "    'customername': 'customer_name',\n",
        "}\n",
        "df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns}, inplace=True)\n",
        "\n",
        "for c in [col for col in ['customer_name','email','phone_num','state','city','category','sub_category','segment','address','order_priority','payment_method','order_status','product_name'] if col in df.columns]:\n",
        "    df[c] = trim_str(df[c])\n",
        "\n",
        "for c in ['customer_name','state','city','category','sub_category','segment','order_status','order_priority','payment_method']:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].str.replace('\\s+', ' ', regex=True).str.title()\n",
        "\n",
        "if 'email' in df.columns:\n",
        "    df['email'] = df['email'].str.lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11de7c55",
      "metadata": {
        "id": "11de7c55"
      },
      "source": [
        "\n",
        "## 7) Normalización de estados y subcategorías (lookups)\n",
        "\n",
        "- Unificamos variantes de **estado** usando `lookup_states`.\n",
        "- Corregimos **typos de subcategoría** con `lookup_subcats`.\n",
        "- Unificamos valores de **segmento** (e.g., `Home-Office` → `Home Office`, `Consumr` → `Consumer`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583510b4",
      "metadata": {
        "id": "583510b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Estados\n",
        "if {'state_variant','state_clean'}.issubset(lookup_states.columns):\n",
        "    lk = lookup_states.copy()\n",
        "    lk['state_variant_norm'] = lk['state_variant'].astype(str).str.strip().str.title().str.replace('\\s+', ' ', regex=True)\n",
        "    state_map = dict(zip(lk['state_variant_norm'], lk['state_clean']))\n",
        "    if 'state' in df.columns:\n",
        "        state_norm = df['state'].astype(str).str.strip().str.title().str.replace('\\s+', ' ', regex=True)\n",
        "        df['state'] = state_norm.map(state_map).fillna(state_norm)\n",
        "\n",
        "# Subcategorías\n",
        "if {'dirty_subcategory','clean_subcategory'}.issubset(lookup_subcats.columns):\n",
        "    sub_map = dict(zip(lookup_subcats['dirty_subcategory'].astype(str).str.title(), lookup_subcats['clean_subcategory']))\n",
        "    if 'sub_category' in df.columns:\n",
        "        df['sub_category'] = df['sub_category'].replace(sub_map)\n",
        "\n",
        "# Segmento\n",
        "if 'segment' in df.columns:\n",
        "    df['segment'] = df['segment'].replace({'Home-Office': 'Home Office', 'Consumr': 'Consumer'})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65131c91",
      "metadata": {
        "id": "65131c91"
      },
      "source": [
        "\n",
        "## 8) Reglas de integridad de negocio\n",
        "\n",
        "- `ship_date >= order_date` y `payment_date >= order_date` → marcamos excepciones.\n",
        "- `qty` mínimo 1; tratamos 0/negativos como inválidos y los reportamos.\n",
        "- Recalculamos `total_calc = (unit_price * qty) * (1 - discount_pct) + tax + shipping` (shipping nulo → 0).\n",
        "- **Flag** si `total` reportado difiere de `total_calc`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dbfdd48",
      "metadata": {
        "id": "5dbfdd48"
      },
      "outputs": [],
      "source": [
        "\n",
        "if 'qty' in df.columns:\n",
        "    df['qty'] = pd.to_numeric(df['qty'], errors='coerce')\n",
        "\n",
        "df['flag_ship_before_order'] = False\n",
        "df['flag_payment_before_order'] = False\n",
        "if {'ship_date','order_date'}.issubset(df.columns):\n",
        "    df['flag_ship_before_order'] = (df['ship_date'] < df['order_date']) & df['ship_date'].notna() & df['order_date'].notna()\n",
        "if {'payment_date','order_date'}.issubset(df.columns):\n",
        "    df['flag_payment_before_order'] = (df['payment_date'] < df['order_date']) & df['payment_date'].notna() & df['order_date'].notna()\n",
        "\n",
        "if 'shipping' in df.columns:\n",
        "    df['shipping'] = df['shipping'].fillna(0.0)\n",
        "if 'discount_pct' in df.columns:\n",
        "    df['discount_pct'] = df['discount_pct'].fillna(0.0)\n",
        "\n",
        "for col in ['unit_price','qty','tax','shipping','discount_pct']:\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0.0\n",
        "\n",
        "df['total_calc'] = (df['unit_price'] * df['qty']) * (1 - df['discount_pct']) + df['tax'].fillna(0) + df['shipping'].fillna(0)\n",
        "\n",
        "if 'total' in df.columns:\n",
        "    df['flag_total_discrepancy'] = (np.round(df['total'],2) != np.round(df['total_calc'],2))\n",
        "else:\n",
        "    df['total'] = df['total_calc']\n",
        "    df['flag_total_discrepancy'] = False\n",
        "\n",
        "df['flag_qty_invalid'] = df['qty'].fillna(0) <= 0\n",
        "print('Resumen flags:')\n",
        "display(df[['flag_ship_before_order','flag_payment_before_order','flag_total_discrepancy','flag_qty_invalid']].sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbeab86",
      "metadata": {
        "id": "2bbeab86"
      },
      "source": [
        "\n",
        "## 9) Geodatos: detección de lat/long intercambiados\n",
        "\n",
        "- Asumimos México: latitudes entre ~14 y 33, longitudes entre ~-118 y -86.\n",
        "- Si detectamos coordenadas fuera de rango pero su inversión cae en rango, **intercambiamos**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d77061",
      "metadata": {
        "id": "03d77061"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fix_latlon(row):\n",
        "    lat, lon = row.get('latitude', np.nan), row.get('longitude', np.nan)\n",
        "    if pd.isna(lat) or pd.isna(lon):\n",
        "        return lat, lon, False\n",
        "    def in_mx(a, b):\n",
        "        return (14 <= a <= 33) and (-118 <= b <= -86)\n",
        "    ok = in_mx(lat, lon)\n",
        "    swapped_ok = in_mx(lon, lat)\n",
        "    if (not ok) and swapped_ok:\n",
        "        return lon, lat, True\n",
        "    return lat, lon, False\n",
        "\n",
        "if {'latitude','longitude'}.issubset(df.columns):\n",
        "    fixed = df.apply(fix_latlon, axis=1, result_type='expand')\n",
        "    df[['latitude','longitude','flag_latlon_swapped']] = fixed\n",
        "    print('Coordenadas corregidas (swaps):', int(df['flag_latlon_swapped'].sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f10b8af",
      "metadata": {
        "id": "0f10b8af"
      },
      "source": [
        "\n",
        "## 10) Duplicados\n",
        "\n",
        "- Definimos llave de granularidad (`order_id`, `productid`, `order_date` si existen).\n",
        "- Eliminamos duplicados conservando la primera ocurrencia y reportamos el impacto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "954be011",
      "metadata": {
        "id": "954be011"
      },
      "outputs": [],
      "source": [
        "\n",
        "keys = [k for k in ['order_id','productid','order_date'] if k in df.columns]\n",
        "before = df.shape[0]\n",
        "if keys:\n",
        "    df = df.sort_values(keys).drop_duplicates(subset=keys, keep='first')\n",
        "after = df.shape[0]\n",
        "print('Duplicados eliminados:', before - after)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce282ea0",
      "metadata": {
        "id": "ce282ea0"
      },
      "source": [
        "\n",
        "## 11) Reporte de calidad (antes/después por regla)\n",
        "\n",
        "Generamos un pequeño panel con:\n",
        "- Nulos por columna\n",
        "- Conteo de flags\n",
        "- Filas afectadas por cada regla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151a5275",
      "metadata": {
        "id": "151a5275"
      },
      "outputs": [],
      "source": [
        "\n",
        "quality = {}\n",
        "quality['nulls_total'] = int(df.isna().sum().sum())\n",
        "\n",
        "for flag in ['flag_ship_before_order','flag_payment_before_order','flag_total_discrepancy','flag_qty_invalid','flag_latlon_swapped']:\n",
        "    if flag in df.columns:\n",
        "        quality[flag] = int(df[flag].sum())\n",
        "\n",
        "quality_df = pd.DataFrame({'metric': list(quality.keys()), 'value': list(quality.values())})\n",
        "print('Resumen de calidad:'); display(quality_df)\n",
        "\n",
        "OUT_CSV = 'fact_orders_clean.csv'\n",
        "OUT_QUALITY = 'quality_report.csv'\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "quality_df.to_csv(OUT_QUALITY, index=False)\n",
        "print(f'Archivos exportados: {OUT_CSV}, {OUT_QUALITY}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939b837f",
      "metadata": {
        "id": "939b837f"
      },
      "source": [
        "\n",
        "## 12) Insights rápidos (visualizaciones)\n",
        "\n",
        "- Ventas (total_calc) por categoría\n",
        "- Margen (profit) por categoría\n",
        "- Distribución de descuento efectivo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50c6581",
      "metadata": {
        "id": "c50c6581"
      },
      "outputs": [],
      "source": [
        "\n",
        "for col in ['total_calc','category','profit','discount_pct','qty']:\n",
        "    if col not in df.columns:\n",
        "        df[col] = np.nan\n",
        "\n",
        "sales_cat = df.groupby('category', dropna=False)['total_calc'].sum().sort_values(ascending=False)\n",
        "sales_cat.plot(kind='bar', title='Ventas por categoría')\n",
        "plt.xlabel('Categoría'); plt.ylabel('Ventas'); plt.show()\n",
        "\n",
        "if 'profit' in df.columns:\n",
        "    profit_cat = df.groupby('category', dropna=False)['profit'].sum().sort_values(ascending=False)\n",
        "    profit_cat.plot(kind='bar', title='Margen por categoría')\n",
        "    plt.xlabel('Categoría'); plt.ylabel('Margen'); plt.show()\n",
        "\n",
        "df['discount_pct'].dropna().plot(kind='hist', bins=20, title='Distribución de descuento efectivo')\n",
        "plt.xlabel('Descuento (fracción)'); plt.ylabel('Frecuencia'); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fcfc900",
      "metadata": {
        "id": "6fcfc900"
      },
      "source": [
        "## 13) Vista previa del dataset limpio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd7bbac",
      "metadata": {
        "id": "2dd7bbac"
      },
      "outputs": [],
      "source": [
        "\n",
        "display(df.head(10))\n",
        "print('Forma final:', df.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}